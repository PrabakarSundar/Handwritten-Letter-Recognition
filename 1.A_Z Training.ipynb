{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from numpy.testing import assert_allclose\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Dataset as Image/1.Train/**/*.png'\n",
    "dataTest = []\n",
    "labelTest = []  \n",
    "cnumo = 0\n",
    "print(\"-->Classifying Images. I't will take some time. Plese Wait.\")\n",
    "for filename in glob.iglob(file_path, recursive=True):\n",
    "    FN = filename.split(\"1.Train\")\n",
    "    FN = FN[1]\n",
    "    #print(FN[1])\n",
    "    cnumo += 1\n",
    "    image = cv2.imread(filename, 1)\n",
    "    image = cv2.resize(image, (28, 28))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = image.reshape(( 28, 28,1))\n",
    "    #image=np.expand_dims(image,axis=2)\n",
    "    dataTest.append (image)\n",
    "    lab = ord(FN[1])-65\n",
    "    labelTest.append(lab)\n",
    "print(\"-->Classification Done\")\n",
    "print(cnumo)\n",
    "dataTest = np.array(dataTest)\n",
    "labelTest = np.array(labelTest)\n",
    "\n",
    "\n",
    "X = dataTest.astype(\"float32\")/255\n",
    "print(X.shape)\n",
    "Y = labelTest\n",
    "print(\"-->Converting Data\")\n",
    "print(\"Number of samples to test: \",len(dataTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# seed for reproducing same results\n",
    "seed = 785\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load dataset(\"Image is stored in CSV file to save image to array conversion time\")\n",
    "print(\"Loading Image File. This will take some time. Please be patient\")\n",
    "dataset = np.loadtxt('Dataset as csv/A_Z Handwritten Data.csv', delimiter=',')'''\n",
    "print(\"Image dataset loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input and output variables\n",
    "#X = dataset[:,0:784]\n",
    "#Y = dataset[:,0]\n",
    "\n",
    "# seed for reproducing same results\n",
    "seed = 785\n",
    "np.random.seed(seed)\n",
    "\n",
    "# split the data into training (50%) and testing (50%)\n",
    "(X_train, X_val, Y_train, Y_val) = train_test_split(X, Y, test_size=0.30, random_state=seed)\n",
    "\n",
    "#convert image resolution to 28x28 image\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_val = X_val / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "\n",
    "num_classes = Y_val.shape[1]\n",
    "print(\"Total Classes:\",num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath = \"model-v3.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=200, callbacks=callbacks_list, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained file and resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run this module if you need to <<Resume Training>>. This is not an incremental training. \n",
    "#This is like resume training with the old data set.\n",
    "\n",
    "#Compile model\n",
    "new_model = load_model(filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "new_model.fit(X_train, Y_train, epochs=0, batch_size=200, callbacks=callbacks_list, validation_data=(X_val, Y_val))\n",
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the loss value & metrics values for the model in test mode.\n",
    "new_model = load_model(filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "scoreEval = new_model.evaluate(X_val, Y_val, verbose=1)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scoreEval[1]*100))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shutdown the kernel after training. \n",
    "    Shutdown the kernel to release the used up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
